# Awesome LLM Papers - ACL 2024

A curated list of interesting papers from ACL 2024, focusing on LLM (Large Language Models) and their applications.

## Fields

### Scientific LLM

- **SciMON: Scientific Inspiration Machines Optimized for Novelty**
- **Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering**
- **Co-training for Low Resource Scientific Natural Language Inference**
- **Training Language Models to Generate Text with Citations via Fine-grained Rewards**
- **Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators**
- **Temporal Knowledge Question Answering via Abstract Reasoning Induction**
- **Synthesizing Text-to-SQL Data from Weak and Strong LLMs**
- **Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval**
- **Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models**
- **SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs**
- **LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression**
- **Making Long-Context Language Models Better Multi-Hop Reasoners**
- **Long-Context Language Modeling with Parallel Context Encoding**
- **LooGLE: Can Long-Context Language Models Understand Long Contexts?**

### LLM Training

- **How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition**
- **Instruction-tuned Language Models are Better Knowledge Learners**
- **Full Parameter Fine-tuning for Large Language Models with Limited Resources**
- **Aligning Large Language Models with Human Preferences through Representation Engineering**
- **Answer is All You Need: Instruction-following Text Embedding via Answering the Question**
- **Learn from Failure: Fine-tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving**
- **Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning**
- **LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin**
- **MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning**
- **Harder Task Needs More Experts: Dynamic Routing in MoE Models**

### Retrieval-Augmented Generation (RAG)

- **Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation**
- **Retrieval-Augmented Multilingual Knowledge Editing**
- **Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation**
- **Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval**
- **Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments**
- **Improving Text Embeddings with Large Language Models**

### Inference

- **Inference to the Best Explanation in Large Language Models**

### Text Extraction

- **TTM-RE: Memory-Augmented Document-Level Relation Extraction**

### Techniques of Interest

- **What Evidence Do Language Models Find Convincing?**
- **Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs**
- **Your Transformer is Secretly Linear**
- **Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models**
- **Multimodal Instruction Tuning with Conditional Mixture of LoRA**
- **Reasoning in Flux: Enhancing Large Language Models Reasoning through Uncertainty-aware Adaptive Guidance**
- **Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models**
- **Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts**
- **Do Llamas Work in English? On the Latent Language of Multilingual Transformers**
- **Context versus Prior Knowledge in Language Models**
- **DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows**
- **Instruction Fusion: Advancing Prompt Evolution through Hybridization**

## Contributing

Feel free to submit a pull request to add more papers or suggest improvements!

---

This list is inspired by the papers presented at ACL 2024 and focuses on large language models and their applications across various domains. Contributions are welcome!
